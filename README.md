# Towards the evaluation of graph embeddings with deep decoders

The Embedding-Eval-Framework is the bachelor thesis project of Paul Wagner.


## Abstract

This thesis presents a method for evaluating graph embeddings based on the performance of decoders. 
Graph embeddings are widely used for different tasks [...]. As such it is of interest to select an embedder, that is 
able to effectively embed graph structures.

The thesis examines how the learning efficiency and precision of decoders can be used as a score on how effectively 
information is stored in an embedding. The idea being, that if an embedding doesn't store its information effectively, 
than a decoder isn't able to learn the embedding (without overfitting). On the contrary, should an embedding store 
information effectively, than a decoder should be able to learn the embedding (without overfitting).

It provides a ready-to-use jupyter notebook to validate presented test results and enables third-parties to test on
other embeddings.

Most embeddings use general vicinity to represent information about connections. A generic decoder has the advantage, 
that it can learn embeddings that use different measures to represent information.


## Introduction

### Motivation

TBD
- Why embeddings? (What are they used for?)
- Differentiate between deterministic and machine learning based algorithms
- We see a difference in dimensionality in the result. Deterministic algorithms are generally low dimensional while ML algorithms produce high dimensional embeddings


### Related Work

TBD


### Outline

TBD


## Preliminaries

### Graphs and Embeddings

A graph is a tuple `G = (V, E)` where `u, v \in V` is a set of nodes and `(v, u) = (u, v) = e \in E` a set of 
(undirected) edges between the nodes. A node represents a piece of information, while edges represent known relations 
between the pieces of information.

An embedder maps the sometimes unbound information vector to a fixed length vector, the feature vector, 
`f \in [0, 1]^n = F` where `n` is the dimension of the embedding. The embedding contains information about the edges in 
the original graph.


### Decoders

The decoder used in the thesis is a binary classificator. Given the features `f_u, f_v \in F` of two embedded nodes 
`u, v \in V`. It maps `[*f_u, *f_v]` to a number `p`, its prediction.

TBD
- loss function
- neural nets
- optimizer
- dataset splitting


## Evaluation

### Setup

The evaluator framework operates in four steps
1. Generate dataset from graph and embedding
2. Split dataset in train-, validation- and test-set
3. Train the model on the train-set while using the validation-set for early stopping
4. Test the model on the test-set to get a general score for the whole embedding

The dataset is split for two reasons
1. Ensure that no overfitting takes place.
   - An overfitted model could give good scores to bad embeddings as long as the embedding is not too complex
2. Reduce dataset size and class imbalance
   - A graph with `n` nodes contains up to `n^2` edges
   - The dataset size is too large for big graphs to loop all data points in each epoch
   - The number of edges in real world (power law) graphs increases [...], resulting in a class imbalance

TBD: graphs with number of edges to graph size of girgs

To mitigate the dataset size and class imbalance problems, the framework uses a new sub-graph for each epoch. 
These sub-graphs can be chosen with a variety of different subsampling methods. To name a few
- Weighted Random Selection
- BFS/DFS
- Random Walk
- Forest Fire

The framework uses a Random Walk with three degrees of freedom
- `size`, the subgraph size 
- `alpha`, the jump probability
- `bordem_pth`, a relative threshold to the subgraph size for jumps

As the `bordem_pth` is chosen based on the task at hand and `size` and `alpha`, it is kept constant and from here on out
not considered a (relevant) degree of freedom. The subsampling method of the framework therefore only consists of two 
(relevant) degrees of freedom.

TBD: subgraph generation source code

TBD
- expect higher `size` results in better representation of graph structure but also higher class imbalance and run time
- expect lower `alpha` results in higher connectivity of components but higher run time
  - *could be different based on embedding used, maybe test on multiple settings (like precision-recall-curve)*
- loss function
- precision-recall-curve and average precision
- f1-score and threshold
- use weights

### Embeddings

#### Random Geometric Graphs

Random geometric graphs are generated by generating random positions for nodes and connecting two nodes, when the 
distance between them is less than a threshold. The feature of a node would be its position (in 2d-space).

Advantages are that the graph can be easily generated and that sub-graphs of random geometric graphs are themselves
random geometric graphs.


TBD
- other characteristics?


#### Girgs

TBD
- see https://github.com/chistopher/girgs
- have power law characteristics
- use position and weight as features
- characteristics?


#### Random Graphs

Random graphs are graphs with structures as shown above but random features. They represent inefficient embeddings.
Random graphs are used to validate, that inefficient embeddings yield bad scores.

### Results

TBD
- *or inline with the different embeddings*


## Conclusion

TBD
