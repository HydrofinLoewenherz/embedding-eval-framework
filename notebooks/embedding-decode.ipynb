{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Code from [scale embedding-decoder](https://git.scc.kit.edu/scale/research/embedding-decoder) with slight changes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using project root as working dir\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")\n",
    "    print(\"using project root as working dir\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from src.map import Map\n",
    "from src.disc import gen_disc_graph, gen_disc_edge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "args = Map(\n",
    "    batch_size = 64,\n",
    "    epochs = 30,\n",
    "    random_seed = None,\n",
    "    graph_size = 1000,\n",
    "    graph_average_degree = 10,\n",
    "    rg_radius = 0.05,\n",
    "    layers = 10,\n",
    "    layer_size = 16,\n",
    "    train_size = 0.7,\n",
    "    wandb = False,\n",
    "    ds_padded = True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "if args.wandb:\n",
    "    wandb.login()\n",
    "    wandb.init(project=\"embedding-eval-framework\", entity=\"hydrofin\")\n",
    "    wandb.run"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def parse_graph(graph):\n",
    "    graph_nodes = nx.nodes(graph)\n",
    "    # all combinations of x and y (with x > y)\n",
    "    node_pairs = [ [i_p0, i_p1] for i_p0 in tqdm(range(graph.number_of_nodes()), desc=\"generating edge pairs\") for i_p1 in range(i_p0 + 1, graph.number_of_nodes()) ]\n",
    "    ds_values = [ [graph_nodes[ei0]['pos'][0], graph_nodes[ei0]['pos'][1], graph_nodes[ei1]['pos'][0], graph_nodes[ei1]['pos'][1]] for [ei0, ei1] in tqdm(node_pairs, desc=\"mapping edge positions\") ]\n",
    "    ds_labels = [ 1 if graph.has_edge(ei0, ei1) else 0 for [ei0, ei1] in tqdm(node_pairs, desc=\"creating labels for edges\") ]\n",
    "    return node_pairs, ds_values, ds_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "## Padding:: Result for now: -> No big difference\n",
    "## Padding:: maybe remove non-edges\n",
    "## Padding:: maybe implement into 'tf.data.Dataset.from_tensor_slices' and normalize batches\n",
    "\n",
    "## run multiple times and average (min 10)\n",
    "## preprocess with data padding (duplicate edges in buckets so that all buckets have same amount of edges)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def pad_dataset(ds_values, ds_labels):\n",
    "    label_diff = ds_labels.count(0) - ds_labels.count(1)\n",
    "    label1_is = [i for i, el in tqdm(enumerate(ds_labels), desc=\"generating duplicates\") if el == 1]\n",
    "    label1_is_sample = random.sample(label1_is, label_diff, counts=([label_diff] * len(label1_is)))\n",
    "    pad_values = ds_values + [ds_values[i] for i in tqdm(label1_is_sample, desc=\"adding duplicates for positions\")]\n",
    "    pad_labels = ds_labels + [ds_labels[i] for i in tqdm(label1_is_sample, desc=\"adding duplicates for labels\")]\n",
    "    return pad_values, pad_labels\n",
    "\n",
    "\n",
    "def prepare_dataset(ds_values, ds_labels):\n",
    "    if args.ds_padded:\n",
    "        pad_values, pad_labels = pad_dataset(ds_values, ds_labels)\n",
    "    else:\n",
    "        pad_values, pad_labels = ds_values, ds_labels\n",
    "\n",
    "    n_values = len(pad_values)\n",
    "\n",
    "    full_dataset = tf.data.Dataset\\\n",
    "        .from_tensor_slices((pad_values, pad_labels))\\\n",
    "        .batch(args.batch_size)\\\n",
    "        .shuffle(np.ceil(n_values / 2))\n",
    "    n_train = int(args.train_size * n_values)\n",
    "    train_dataset = full_dataset.take(n_train)\n",
    "    test_dataset = full_dataset.skip(n_train)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def run_model(train_dataset, test_dataset):\n",
    "    # build model\n",
    "    model_array = [tf.keras.layers.InputLayer(input_shape=4)]\n",
    "    for i in range(args.layers):\n",
    "        model_array.append(tf.keras.layers.Dense(args.layer_size, activation='relu'))\n",
    "    model_array.append(tf.keras.layers.Flatten())\n",
    "    model_array.append(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    dense_model = tf.keras.Sequential(model_array)\n",
    "    dense_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),  # TODO try other loss function\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Recall(thresholds=0),\n",
    "            tf.keras.metrics.AUC(\n",
    "                curve=\"PR\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    callbacks = []\n",
    "    if args.wandb:\n",
    "        callbacks.append(WandbCallback())\n",
    "    # run model\n",
    "    dense_model.fit(train_dataset, epochs=args.epochs, callbacks=callbacks, verbose=1)\n",
    "    eval_result = list(dense_model.evaluate(test_dataset, verbose=1)) # list(loss, acc, recall, auc)\n",
    "    return dense_model, eval_result\n",
    "\n",
    "## iterate over un-padded edges and calculate https://discord.com/channels/934839185855086662/988688735161946144/1070345614782632017\n",
    "## - does the padding change the result?\n",
    "## - weight edges higher than non-edges\n",
    "## - use as loss function and metric\n",
    "\n",
    "## mathematically define the BinaryCrossentropy in this model\n",
    "## - used to compare to fastgae\n",
    "## - compare to other loss functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating edge pairs: 100%|██████████| 1000/1000 [00:00<00:00, 1786.98it/s]\n",
      "mapping edge positions: 100%|██████████| 499500/499500 [00:00<00:00, 586821.00it/s]\n",
      "creating labels for edges: 100%|██████████| 499500/499500 [00:00<00:00, 2720359.37it/s]\n",
      "generating duplicates: 499500it [00:00, 4757282.42it/s]\n",
      "adding duplicates for positions: 100%|██████████| 489750/489750 [00:00<00:00, 4604698.00it/s]\n",
      "adding duplicates for labels: 100%|██████████| 489750/489750 [00:00<00:00, 5364029.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15458/15458 [==============================] - 22s 1ms/step - loss: 0.0618 - accuracy: 0.9730 - recall_1: 1.0000 - auc_1: 0.9942\n",
      "Epoch 2/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0222 - accuracy: 0.9942 - recall_1: 1.0000 - auc_1: 0.9977\n",
      "Epoch 3/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0176 - accuracy: 0.9954 - recall_1: 1.0000 - auc_1: 0.9983\n",
      "Epoch 4/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0153 - accuracy: 0.9960 - recall_1: 1.0000 - auc_1: 0.9987\n",
      "Epoch 5/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0141 - accuracy: 0.9963 - recall_1: 1.0000 - auc_1: 0.9988\n",
      "Epoch 6/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0134 - accuracy: 0.9965 - recall_1: 1.0000 - auc_1: 0.9989\n",
      "Epoch 7/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0125 - accuracy: 0.9967 - recall_1: 1.0000 - auc_1: 0.9990\n",
      "Epoch 8/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0116 - accuracy: 0.9969 - recall_1: 1.0000 - auc_1: 0.9991\n",
      "Epoch 9/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0122 - accuracy: 0.9968 - recall_1: 1.0000 - auc_1: 0.9990\n",
      "Epoch 10/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0113 - accuracy: 0.9971 - recall_1: 1.0000 - auc_1: 0.9991\n",
      "Epoch 11/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0107 - accuracy: 0.9972 - recall_1: 1.0000 - auc_1: 0.9991\n",
      "Epoch 12/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0103 - accuracy: 0.9973 - recall_1: 1.0000 - auc_1: 0.9992\n",
      "Epoch 13/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0099 - accuracy: 0.9974 - recall_1: 1.0000 - auc_1: 0.9993\n",
      "Epoch 14/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0101 - accuracy: 0.9974 - recall_1: 1.0000 - auc_1: 0.9992\n",
      "Epoch 15/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0098 - accuracy: 0.9975 - recall_1: 1.0000 - auc_1: 0.9993\n",
      "Epoch 16/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0102 - accuracy: 0.9974 - recall_1: 1.0000 - auc_1: 0.9993\n",
      "Epoch 17/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0096 - accuracy: 0.9975 - recall_1: 1.0000 - auc_1: 0.9993\n",
      "Epoch 18/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0092 - accuracy: 0.9976 - recall_1: 1.0000 - auc_1: 0.9994\n",
      "Epoch 19/30\n",
      "15458/15458 [==============================] - 22s 1ms/step - loss: 0.0125 - accuracy: 0.9974 - recall_1: 1.0000 - auc_1: 0.9993\n",
      "Epoch 20/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0094 - accuracy: 0.9976 - recall_1: 1.0000 - auc_1: 0.9993\n",
      "Epoch 21/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0081 - accuracy: 0.9979 - recall_1: 1.0000 - auc_1: 0.9994\n",
      "Epoch 22/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0085 - accuracy: 0.9978 - recall_1: 1.0000 - auc_1: 0.9993\n",
      "Epoch 23/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0086 - accuracy: 0.9977 - recall_1: 1.0000 - auc_1: 0.9994\n",
      "Epoch 24/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0111 - accuracy: 0.9977 - recall_1: 1.0000 - auc_1: 0.9993\n",
      "Epoch 25/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0088 - accuracy: 0.9978 - recall_1: 1.0000 - auc_1: 0.9993\n",
      "Epoch 26/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0083 - accuracy: 0.9978 - recall_1: 1.0000 - auc_1: 0.9995\n",
      "Epoch 27/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0227 - accuracy: 0.9974 - recall_1: 1.0000 - auc_1: 0.9992\n",
      "Epoch 28/30\n",
      "15458/15458 [==============================] - 21s 1ms/step - loss: 0.0085 - accuracy: 0.9978 - recall_1: 1.0000 - auc_1: 0.9994\n",
      "Epoch 29/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0095 - accuracy: 0.9978 - recall_1: 1.0000 - auc_1: 0.9993\n",
      "Epoch 30/30\n",
      "15458/15458 [==============================] - 20s 1ms/step - loss: 0.0395 - accuracy: 0.9973 - recall_1: 1.0000 - auc_1: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paul\\PycharmProjects\\embedding-eval-framework\\venv\\lib\\site-packages\\keras\\utils\\generic_utils.py:239: RuntimeWarning: divide by zero encountered in log10\n",
      "  numdigits = int(np.log10(self.target)) + 1\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "cannot convert float infinity to integer",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOverflowError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21916\\1739516645.py\u001B[0m in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[0mnode_pairs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mds_values\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mds_labels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparse_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgraph\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mds_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mds_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprepare_dataset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mds_values\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mds_labels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m     \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrun_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mds_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mds_test\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m     \u001B[0mresults\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m     \u001B[0mmodels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgraph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnode_positions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnode_pairs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mds_values\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mds_labels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_21916\\4105124738.py\u001B[0m in \u001B[0;36mrun_model\u001B[1;34m(train_dataset, test_dataset)\u001B[0m\n\u001B[0;32m     50\u001B[0m     \u001B[1;31m# run model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[0mdense_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 52\u001B[1;33m     \u001B[0meval_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdense_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mevaluate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_dataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# list(loss, acc, recall, auc)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     53\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mdense_model\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meval_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\embedding-eval-framework\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     68\u001B[0m             \u001B[1;31m# To get the full stack trace, call:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     69\u001B[0m             \u001B[1;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 70\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     71\u001B[0m         \u001B[1;32mfinally\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m             \u001B[1;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\embedding-eval-framework\\venv\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001B[0m in \u001B[0;36mupdate\u001B[1;34m(self, current, values, finalize)\u001B[0m\n\u001B[0;32m    237\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    238\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 239\u001B[1;33m                 \u001B[0mnumdigits\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlog10\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    240\u001B[0m                 \u001B[0mbar\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m\"%\"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnumdigits\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\"d/%d [\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mcurrent\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    241\u001B[0m                 \u001B[0mprog\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcurrent\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOverflowError\u001B[0m: cannot convert float infinity to integer"
     ]
    }
   ],
   "source": [
    "# run multiple times\n",
    "results = []\n",
    "models = []\n",
    "for iteration in range(3):\n",
    "    print(f'starting iteration {iteration}')\n",
    "    # generate graph\n",
    "    graph, node_positions, _ = gen_disc_graph(args.graph_size, args.graph_average_degree, args.rg_radius)\n",
    "    # create & run model\n",
    "    node_pairs, ds_values, ds_labels = parse_graph(graph)\n",
    "    ds_train, ds_test = prepare_dataset(ds_values, ds_labels)\n",
    "    model, result = run_model(ds_train, ds_test)\n",
    "    results.append(result)\n",
    "    models.append((model, graph, node_positions, node_pairs, ds_values, ds_labels))\n",
    "\n",
    "losses, accs, recalls, aucs = zip(*results)\n",
    "\n",
    "print(f'finished training models')\n",
    "print(f'avg_loss: {np.average(losses)}  std_loss: {np.std(losses)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get best model\n",
    "(model, graph, node_positions, node_pairs, ds_values, ds_labels) = models[(losses.index(min(losses)))]\n",
    "predictions = [pred[0] for pred in model.predict(ds_values)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print best model\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# print original graph\n",
    "ax[0].set_axis_off()\n",
    "ax[0].set_aspect('equal')\n",
    "ax[0].set_title(\"original graph\")\n",
    "nx.draw_networkx(graph, node_positions, ax=ax[0], node_size=5, with_labels=False, labels={})\n",
    "\n",
    "# generate predict graph\n",
    "threshold = 0.1\n",
    "colors_filtered = np.array([pred for pred in tqdm(predictions, desc=f'generating colors') if pred > threshold])\n",
    "colormap = sns.color_palette(\"flare\", as_cmap=True)\n",
    "pred_graph = nx.Graph()\n",
    "pred_node_pairs = [edge for i, edge in enumerate(node_pairs) if predictions[i] > threshold]\n",
    "pred_graph.add_edges_from(pred_node_pairs)\n",
    "\n",
    "# print predicted graph\n",
    "ax[1].set_axis_off()\n",
    "ax[1].set_aspect('equal')\n",
    "ax[1].set_title(\"reconstructed graph\")\n",
    "nx.draw_networkx(pred_graph, node_positions, ax=ax[1], node_size=5, with_labels=False, labels={}, edge_color=colors_filtered, edge_cmap=colormap)\n",
    "\n",
    "# add color bar for predictions\n",
    "cax = fig.add_axes([ax[1].get_position().x1 + 0.01, ax[1].get_position().y0, 0.02, ax[1].get_position().height])\n",
    "fig.colorbar(mpl.cm.ScalarMappable(cmap=colormap), cax=cax, label=\"confidence\")\n",
    "\n",
    "plt.savefig('./filename.png', dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# OLD"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if args.wandb:\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Additional Plots\n",
    "\n",
    "Additional plots of information about the decoder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "distances = [math.dist([px, py], [qx, qy]) for [px, py, qx, qy] in ds_edges_pos]\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(distances, edge_prediction, s=0.01)\n",
    "plt.show()\n",
    "## smaller points\n",
    "## also with lines between (has to be sorted first)\n",
    "## plot into #of-edges per distance (see how much data/information the nn gets per distance)\n",
    "## more points (for more information around threshold distance)\n",
    "\n",
    "## get threshold back with ml/wsk-theory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist2d([dist for dist in distances], edge_prediction, bins=(np.arange(0, 1, 0.01), np.arange(0, 1, 0.01)))\n",
    "ax.set(xlim=(0, 1), ylim=(0, 1))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_distances = np.arange(0, 1, 0.01)\n",
    "dist_dup = 10\n",
    "test_edges = [(d, gen_disc_edge(d)) for d in np.tile(test_distances, dist_dup)]\n",
    "test_edges"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_predictions = dense_model.predict([edge for (_, edge) in test_edges]) # cartesian\n",
    "test_edge_prediction = [pred[0] for pred in test_predictions]\n",
    "len(test_edges), len(test_edge_prediction)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter([d for (d, _) in test_edges], test_edge_prediction)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_dist, sorted_pred = zip(*sorted(zip([d for (d, _) in test_edges], test_edge_prediction), key = lambda x: x[0]))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(sorted_dist, sorted_pred, marker='o', linewidth=1, markersize=3)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist2d([d for (d, _) in test_edges], test_edge_prediction, bins=(np.arange(0, 1, 0.1), np.arange(0, 1, 0.1)))\n",
    "ax.set(xlim=(0, 1), ylim=(0, 1))\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
