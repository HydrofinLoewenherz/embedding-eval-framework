{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using project root as working dir\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")\n",
    "    print(\"using project root as working dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    random_seed = None\n",
    "    # torch\n",
    "    batch_size = 64\n",
    "    epochs = 10\n",
    "    layers = 10\n",
    "    layer_size = 16\n",
    "    train_size = 0.7\n",
    "    wandb = False\n",
    "    # graph\n",
    "    graph_size = 1000\n",
    "    graph_shape = 'disc'\n",
    "    rg_radius = 0.05\n",
    "    # dataset manipulation\n",
    "    ds_padded = True\n",
    "\n",
    "args = Args()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(f\"using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "NodePosition = Tuple[float, float]\n",
    "NodePositions = List[NodePosition]\n",
    "NodeIndexPairs = List[Tuple[int, int]]\n",
    "\n",
    "def gen_nodes(size: int, shape: str = \"disc\") -> NodePositions:\n",
    "    if shape == 'disc':\n",
    "        return __gen_nodes_disc(size)\n",
    "    else:\n",
    "        raise f'unsupported node shape: {shape}'\n",
    "\n",
    "\n",
    "def __gen_nodes_disc(amount: int) -> NodePositions:\n",
    "    points = []\n",
    "    with tqdm(total=amount, desc=\"generating random-uniform nodes on disc\") as pbar:\n",
    "        while len(points) < amount:\n",
    "            p = (random.uniform(0, 1), random.uniform(0, 1))\n",
    "            d = (p[0] - 0.5, p[1] - 0.5)\n",
    "            if math.sqrt(d[0] * d[0] + d[1] * d[1]) > 0.5:\n",
    "                continue\n",
    "            points.append(p)\n",
    "            pbar.update(1)\n",
    "    return points\n",
    "\n",
    "\n",
    "def get_node_pairs(n_nodes: int) -> NodeIndexPairs:\n",
    "    return [\n",
    "        (i0, i1)\n",
    "        for i0 in tqdm(range(n_nodes), desc=\"generating node pairs\")\n",
    "        for i1 in range(i0 + 1, n_nodes)\n",
    "    ]\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/36460020/10619052\n",
    "def list_to_dict(items: list) -> dict:\n",
    "    return {v: k for v, k in enumerate(tqdm(items, desc=\"creating dict from list\"))}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Define graph builder\n",
    "class RandomGeometricGraphBuilder:\n",
    "    def __init__(self):\n",
    "        # generate graph\n",
    "        self.nodes = gen_nodes(args.graph_size, args.graph_shape)\n",
    "        self.n_nodes = len(self.nodes)\n",
    "        self.graph = nx.random_geometric_graph(\n",
    "            self.n_nodes,\n",
    "            args.rg_radius,\n",
    "            pos=list_to_dict(self.nodes)\n",
    "        )\n",
    "        self.node_index_pairs = get_node_pairs(self.n_nodes)\n",
    "        self.edges: NodeIndexPairs = [\n",
    "            (i0, i1)\n",
    "            for (i0, i1) in tqdm(self.node_index_pairs, desc=\"generating dataset labels from node pairs\")\n",
    "            if self.graph.has_edge(i0, i1)\n",
    "        ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(4, args.layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(args.layer_size, args.layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(args.layer_size, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Define evaluator\n",
    "class EmbeddingEvaluator:\n",
    "    def __init__(self, nodes: NodePositions, edges: NodeIndexPairs, embedding: NodePositions):\n",
    "        self.nodes = nodes # used for 2D representation of graph (not used in training)\n",
    "        self.n_nodes = len(nodes)\n",
    "        self.node_index_pairs = get_node_pairs(self.n_nodes)\n",
    "        self.edges = edges\n",
    "        self.embedding = embedding\n",
    "        # generate net\n",
    "        self.reset_net()\n",
    "        # generate dataset\n",
    "        self.ds_values = torch.tensor([\n",
    "            [*self.nodes[i0], *self.nodes[i1]] # type: [float, float, float, float]\n",
    "            for (i0, i1) in tqdm(self.node_index_pairs, desc=\"generating dataset values from node pairs\")\n",
    "        ])\n",
    "        self.ds_labels = torch.LongTensor([\n",
    "            1 if (edge in self.edges) else 0\n",
    "            for edge in tqdm(self.node_index_pairs, desc=\"generating dataset labels from node pairs\")\n",
    "        ])\n",
    "        self.dataset = TensorDataset(self.ds_values, self.ds_labels)\n",
    "        #? do we ant to over-fit?\n",
    "        self.train_dataset, self.test_dataset = torch.utils.data.random_split(self.dataset, [args.train_size, 1 - args.train_size])\n",
    "        self.train_dataloader = DataLoader(self.train_dataset, batch_size=args.batch_size, num_workers=0, shuffle=True)\n",
    "        self.test_dataloader = DataLoader(self.test_dataset, batch_size=args.batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "    def reset_net(self):\n",
    "        self.net = NeuralNetwork().to(device)\n",
    "\n",
    "\n",
    "    def train(self, loss_fn, optimizer):\n",
    "        for epoch in range(args.epochs):\n",
    "            with tqdm(total=len(self.train_dataloader), desc=\"starting model...\") as pbar:\n",
    "                pbar.set_description(f\"Epoch {epoch + 1}\")\n",
    "                self.__train(pbar, loss_fn, optimizer)\n",
    "                self.__test(pbar, loss_fn)\n",
    "\n",
    "\n",
    "    def __train(self, pbar, loss_fn, optimizer):\n",
    "        self.net.train()\n",
    "\n",
    "        n_train_batches = len(self.train_dataloader)\n",
    "        intv = int(n_train_batches / 100) # interval in which the pbar is updated (every 1%)\n",
    "        for batch, (X, y) in enumerate(self.train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Compute prediction error\n",
    "            pred = self.net(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # update progress\n",
    "            if batch % intv == 0 or batch == n_train_batches - 1:\n",
    "                pbar.update(batch - pbar.n)\n",
    "                pbar.set_postfix_str(f\"loss: {loss.item():>6f}\")\n",
    "\n",
    "\n",
    "    def __test(self, pbar, loss_fn):\n",
    "        self.net.eval()\n",
    "\n",
    "        n_test_batches = len(self.test_dataloader)\n",
    "        n_test_values = len(self.test_dataloader.dataset)\n",
    "        test_loss, correct = 0, 0\n",
    "        pbar.set_postfix_str(f\"evaluating epoch...\")\n",
    "        with torch.no_grad():\n",
    "            for x, y in self.test_dataloader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                pred = self.net(x)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= n_test_batches\n",
    "        correct /= n_test_values\n",
    "        pbar.set_postfix_str(f\"epoch result: accuracy: {(100*correct):>0.1f}%, avg_loss: {test_loss:>8f}\")\n",
    "\n",
    "\n",
    "    def predict(self, embedding: NodePositions):\n",
    "        self.net.eval()\n",
    "\n",
    "        data = TensorDataset(torch.tensor(embedding))\n",
    "        dataloader = DataLoader(data, num_workers=0, shuffle=False)\n",
    "\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for (x) in dataloader:\n",
    "                x = x.to(device)\n",
    "                pred = self.net(x)\n",
    "                predictions.push(pred.cpu().detatch().numpy())\n",
    "\n",
    "        return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "generating random-uniform nodes on disc:   0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "271479e511934119b9838f7b4e3b56a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "creating dict from list:   0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb7ed8ce74f04ef8a3c014265e429dab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generating node pairs:   0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6973bfabab444e4ab81a54f8cafe12fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generating dataset labels from node pairs:   0%|          | 0/499500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3a7fcf4db9e4ddfa2d4fa3dcf334311"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generating node pairs:   0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c888ee26833e470e86f01f66693d1149"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generating dataset values from node pairs:   0%|          | 0/499500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5f3c74598d34783b95b96b806637094"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generating dataset labels from node pairs:   0%|          | 0/499500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1c3a2655abc4b3ab0e4720ec8a032b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "starting model...:   0%|          | 0/5464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59f0e1777e5f4672a5b726f3a27b28e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "starting model...:   0%|          | 0/5464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aabfc4d0ce4c416e8a0711d8842ecc38"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "starting model...:   0%|          | 0/5464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48221980d7054a31844ed72209f757a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "starting model...:   0%|          | 0/5464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c69eba6ac2a9454c835a1858c4ac6b31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "starting model...:   0%|          | 0/5464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5996d8887804e53a9569f5c1e22e471"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "starting model...:   0%|          | 0/5464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd19e4bcfc7547dbbe1d3e61255a67da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "starting model...:   0%|          | 0/5464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64ef49dd2d9e49b2b6e20b4b183f6dd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "starting model...:   0%|          | 0/5464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "734cfe2103e745d68413d4fa6ffdf72d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "starting model...:   0%|          | 0/5464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "581f5d58a8144069aa36ad4d0ea85462"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "starting model...:   0%|          | 0/5464 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3df254a5cce240688bfc4af9db002f8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "graph_builder = RandomGeometricGraphBuilder()\n",
    "evaluator = EmbeddingEvaluator(\n",
    "    graph_builder.nodes,\n",
    "    graph_builder.edges,\n",
    "    graph_builder.nodes # for random geometric graph, the structure defines the embedding\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(evaluator.net.parameters(), lr=1e-3)\n",
    "\n",
    "evaluator.train(loss_fn, optimizer)\n",
    "\n",
    "#pred = evaluator.predict(graph_builder.nodes)\n",
    "#print(pred)\n",
    "\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "predicting:   0%|          | 0/499500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c240e24fa8c449f99f71bb58f2027fac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(self, edges: NodeIndexPairs):\n",
    "    self.net.eval()\n",
    "\n",
    "    #data = TensorDataset(torch.tensor(edges))\n",
    "    data = edges\n",
    "    dataloader = DataLoader(data, num_workers=0, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for x in tqdm(dataloader, desc=\"predicting\"):\n",
    "            pred = self.net(x.to(device))\n",
    "            predictions.append(pred)\n",
    "        #return self.net(edges.to(device))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "pred = predict(evaluator, evaluator.ds_values)\n",
    "print(\"prediced\")\n",
    "\n",
    "#evaluator.net(torch.tensor(graph_builder.nodes).to(device))\n",
    "\n",
    "print(type(pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[ 2.6341, -2.2398]], device='cuda:0')"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(pred[0][0][0]))\n",
    "pred[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for x in dataloader:\n",
    "            pred = self.net(x.to(device))\n",
    "            predictions.append(pred.cpu().detatch().numpy())\n",
    "\n",
    "\n",
    "args = Args()\n",
    "\n",
    "graph_dataset = GraphDataset(args)\n",
    "full_dataset = graph_dataset.dataset\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [args.train_size, 1 - args.train_size]) ## do we ant to over-fit?\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=0, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# train & test\n",
    "for epoch in range(args.epochs):\n",
    "    with tqdm(total=len(train_dataloader), desc=\"starting model...\") as pbar:\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}\")\n",
    "        train(pbar, model, train_dataloader, loss_fn, optimizer)\n",
    "        test(pbar, model, test_dataloader, loss_fn)\n",
    "\n",
    "\n",
    "# predict & print\n",
    "pred = predict(model, graph_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
