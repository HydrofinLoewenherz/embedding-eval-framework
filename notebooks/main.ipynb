{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using project root as working dir\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd().endswith(\"notebooks\"):\n",
    "    os.chdir(\"..\")\n",
    "    print(\"using project root as working dir\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Tuple\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.classification import BinaryConfusionMatrix, BinaryPrecisionRecallCurve, BinaryAveragePrecision\n",
    "import itertools\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    random_seed = None\n",
    "    # torch\n",
    "    batch_size = 10\n",
    "    epochs = 100\n",
    "    layers = 10\n",
    "    layer_size = 16\n",
    "    train_size = 0.8\n",
    "    eval_epochs = True\n",
    "    # graph\n",
    "    graph_size = 800\n",
    "    graph_shape = 'disc'\n",
    "    rg_radius = 0.05\n",
    "    subsample = \"bfs\"\n",
    "    subsample_size = 100\n",
    "    subsample_ranking = \"node_degree\"\n",
    "    balance = False\n",
    "\n",
    "    def print(self):\n",
    "        return f\"\"\"\n",
    "            random_seed = {self.random_seed}\n",
    "            # torch\n",
    "            batch_size = {self.batch_size}\n",
    "            epochs = {self.epochs}\n",
    "            layers = {self.layers}\n",
    "            layer_size = {self.layer_size}\n",
    "            train_size = {self.train_size}\n",
    "            eval_epochs = {self.eval_epochs}\n",
    "            # graph\n",
    "            graph_size = {self.graph_size}\n",
    "            graph_shape = {self.graph_shape}\n",
    "            rg_radius = {self.rg_radius}\n",
    "            subsample = {self.subsample}\n",
    "            subsample_size = {self.subsample_size}\n",
    "            subsample_ranking = {self.subsample_ranking}\n",
    "            balance = {self.balance}\n",
    "        \"\"\"\n",
    "\n",
    "args = Args()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"using {device} device\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "NodePosition = Tuple[float, float]\n",
    "NodePositions = List[NodePosition]\n",
    "NodeIndexPairs = List[Tuple[int, int]]\n",
    "\n",
    "def gen_nodes(size: int, shape: str = \"disc\") -> NodePositions:\n",
    "    if shape == 'disc':\n",
    "        return __gen_nodes_disc(size)\n",
    "    else:\n",
    "        raise f'unsupported node shape: {shape}'\n",
    "\n",
    "\n",
    "def __gen_nodes_disc(amount: int) -> NodePositions:\n",
    "    points = []\n",
    "    with tqdm(total=amount, desc=\"generating random-uniform nodes on disc\") as pbar:\n",
    "        while len(points) < amount:\n",
    "            p = (random.uniform(0, 1), random.uniform(0, 1))\n",
    "            d = (p[0] - 0.5, p[1] - 0.5)\n",
    "            if math.sqrt(d[0] * d[0] + d[1] * d[1]) > 0.5:\n",
    "                continue\n",
    "            points.append(p)\n",
    "            pbar.update(1)\n",
    "    return points\n",
    "\n",
    "# generates an edge with a specified distance on a unit disc\n",
    "# the distance has to be in (0, 1).\n",
    "def gen_disc_edge(d: float) -> [float, float, float, float]:\n",
    "    # loop with p in case p is badly chosen\n",
    "    while True:\n",
    "        px, py = random.uniform(0, 1), random.uniform(0, 1)\n",
    "        v = np.random.random(2) # random direction\n",
    "        vd = (v / np.linalg.norm(v)) * d\n",
    "        [qx, qy] = [px, py] + vd\n",
    "        if math.dist([px, py], [0.5, 0.5]) <= 0.5 and math.dist([qx, qy], [0.5, 0.5]) <= 0.5:\n",
    "            return [px, py, qx, qy]\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/36460020/10619052\n",
    "def list_to_dict(items: list) -> dict:\n",
    "    return {v: k for v, k in enumerate(items)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Define graph builder\n",
    "class RandomGeometricGraphBuilder:\n",
    "    def __init__(self):\n",
    "        # generate graph\n",
    "        self.nodes = gen_nodes(args.graph_size, args.graph_shape)\n",
    "        self.n_nodes = len(self.nodes)\n",
    "        self.graph = nx.random_geometric_graph(\n",
    "            self.n_nodes,\n",
    "            args.rg_radius,\n",
    "            pos=list_to_dict(self.nodes)\n",
    "        )\n",
    "        self.adjacency_matrix = nx.adjacency_matrix(self.graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(4, args.layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(args.layer_size, args.layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(args.layer_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph Subsampling\n",
    "\n",
    "Graph subsampling allows the model to train on a smaller set of nodes (and edges).\n",
    "As the model learns based on all node pairs and the number of pairs strongly increases with the number of nodes, subsampling will (in most cases) provide a big speedup.\n",
    "\n",
    "**Technics**\n",
    "- random (sized,ranked)\n",
    "- random walk (sized,ranked,jump)\n",
    "- bfs (sized)\n",
    "- dfs (sized)\n",
    "- forest fire (flammability)\n",
    "\n",
    "**Node Rankings**\n",
    "- uniform\n",
    "- node degree\n",
    "- page rank\n",
    "- sizes of weakly connected components (`wcc`)\n",
    "- sizes of strongly connected components (`scc`)\n",
    "- Hop-plot\n",
    "- Hop-plot on the largest `wcc`\n",
    "- clustering coefficient (`C_d`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Graph helper class\n",
    "class Graph:\n",
    "    def __init__(self, nodes: NodePositions, adjacency_matrix):\n",
    "        self.nodes = nodes\n",
    "        self.adjacency_matrix = adjacency_matrix\n",
    "        self._precalculated = []\n",
    "\n",
    "        # general pre-calculations (less expensive)\n",
    "        self.n_nodes = len(nodes)\n",
    "        self.node_index_pairs = list(itertools.combinations(range(self.n_nodes), 2))\n",
    "        self.node_degrees = self.adjacency_matrix.sum(axis=0)\n",
    "        self.uniform_weights = [1]*self.n_nodes\n",
    "\n",
    "        # expansive pre-calculations placeholder, these are calculated as needed\n",
    "        self.jump_weights = {}\n",
    "        self.walk_weights = {}\n",
    "\n",
    "    def precalculate(self, node_ranking):\n",
    "        # validate that not already calculated and valid\n",
    "        if node_ranking in self._precalculated:\n",
    "            return\n",
    "        elif node_ranking not in [\"node_degree\", \"uniform\"]:\n",
    "            raise ValueError(f\"unsupported node ranking '{node_ranking}'\")\n",
    "        self._precalculated.append(node_ranking)\n",
    "        print(f\"precalculating {node_ranking}\")\n",
    "\n",
    "        # calculate jump weights\n",
    "        self.jump_weights = {}\n",
    "        if node_ranking == \"node_degree\":\n",
    "            self.jump_weights[\"node_degree\"] = self.node_degrees\n",
    "        elif node_ranking == \"uniform\":\n",
    "            self.jump_weights[\"uniform\"] = self.uniform_weights\n",
    "\n",
    "        # calculate weights for walking the graph\n",
    "        if node_ranking == \"node_degree\":\n",
    "            self.walk_weights[\"node_degree\"] = [\n",
    "                l.todense()[0] if (l := self.node_degrees[i] * self.adjacency_matrix.getrow(i)).sum() > 0 else self.jump_weights\n",
    "                for i in range(self.n_nodes)\n",
    "            ]\n",
    "        elif node_ranking == \"uniform\":\n",
    "            self.walk_weights[\"uniform\"] = [\n",
    "                l.todense()[0] if (l := self.uniform_weights[i] * self.adjacency_matrix.getrow(i)).sum() > 0 else self.jump_weights\n",
    "                for i in range(self.n_nodes)\n",
    "            ]\n",
    "\n",
    "    def subgraph(self, alg: str, node_ranking: str, size: int) -> List[int]:\n",
    "        if alg == \"random\":\n",
    "            return self.random_subgraph(node_ranking, size)\n",
    "        elif alg == \"random_walk\":\n",
    "            return self.random_walk_subgraph(node_ranking, size)\n",
    "        elif alg == \"bfs\":\n",
    "            return self.bfs_subgraph(node_ranking, size)\n",
    "        else:\n",
    "            raise ValueError(f\"unsupported subgraph algorithm '{alg}'\")\n",
    "\n",
    "\n",
    "    def random_subgraph(self, node_ranking: str, size: int, replacement: bool = False):\n",
    "        self.precalculate(node_ranking)\n",
    "        # random sampling is like doing random jumps\n",
    "        return list(WeightedRandomSampler(\n",
    "            self.jump_weights[node_ranking],\n",
    "            size,\n",
    "            replacement=replacement\n",
    "        ))\n",
    "\n",
    "\n",
    "    # TODO add random jumps\n",
    "    def random_walk_subgraph(self, node_ranking: str, size: int):\n",
    "        self.precalculate(node_ranking)\n",
    "        def walk(start: int, n: int):\n",
    "            visited = [start]\n",
    "            yield start\n",
    "            curr = start\n",
    "            while len(visited) < n:\n",
    "                curr = random.choices(\n",
    "                    range(self.n_nodes),\n",
    "                    weights=self.walk_weights[node_ranking][curr]\n",
    "                )[0]\n",
    "                if curr in visited:\n",
    "                    continue\n",
    "                visited.append(curr)\n",
    "                yield curr\n",
    "        return list(walk(\n",
    "            random.choices(\n",
    "                range(self.n_nodes),\n",
    "                weights=self.jump_weights[node_ranking]\n",
    "            )[0],\n",
    "            size\n",
    "        ))\n",
    "\n",
    "\n",
    "    # TODO add random jumps\n",
    "    def bfs_subgraph(self, node_ranking: str, size: int):\n",
    "        self.precalculate(node_ranking)\n",
    "        def bfs(start: int, n: int):\n",
    "            visit = [start]\n",
    "            visited = []\n",
    "            while len(visited) < n:\n",
    "                if len(visit) == 0:\n",
    "                    rand = random.choices(\n",
    "                        range(self.n_nodes),\n",
    "                        weights=self.jump_weights[node_ranking]\n",
    "                    )[0]\n",
    "                    visit.append(rand)\n",
    "                    print(\"bfs: random jump\")\n",
    "                curr = visit.pop(0)\n",
    "                if curr in visited:\n",
    "                    continue\n",
    "                visited.append(curr)\n",
    "                yield curr\n",
    "                visit.extend([\n",
    "                    n\n",
    "                    for n in range(self.n_nodes)\n",
    "                    if self.adjacency_matrix[curr, n] == 1\n",
    "                ])\n",
    "        return list(bfs(\n",
    "            random.choices(\n",
    "                range(self.n_nodes),\n",
    "                weights=self.jump_weights[node_ranking]\n",
    "            )[0],\n",
    "            size\n",
    "        ))\n",
    "\n",
    "\n",
    "    #? sample: forest fire"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "generating random-uniform nodes on disc:   0%|          | 0/800 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42b8dc7bf7a945f683c13804145a7425"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculating uniform\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "graph_builder = RandomGeometricGraphBuilder()\n",
    "graph = Graph(\n",
    "    graph_builder.nodes,\n",
    "    graph_builder.adjacency_matrix\n",
    ")\n",
    "#graph.precalculate(\"uniform\")\n",
    "print(len(set(graph.random_walk_subgraph(\"uniform\", 250))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Define evaluator\n",
    "class EmbeddingEvaluator:\n",
    "    def __init__(self, graph: Graph, writer_log_dir = None):\n",
    "        self.graph = graph\n",
    "        # complete dataset\n",
    "        values = [ e.flatten().tolist() for e in np.array(self.graph.nodes)[self.graph.node_index_pairs]]\n",
    "        labels = [ int(self.graph.adjacency_matrix[s]) for s in self.graph.node_index_pairs ]\n",
    "        self.n_labels = len(labels)\n",
    "        self.n_labels_0 = labels.count(0)\n",
    "        self.ds_values = torch.FloatTensor(values).to(device)\n",
    "        self.ds_labels = torch.FloatTensor(labels).to(device)\n",
    "        self.dataset = TensorDataset(self.ds_values, self.ds_labels)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=args.batch_size, shuffle=True)\n",
    "        # generate net\n",
    "        self.net = NeuralNetwork().to(device)\n",
    "        # tensorboard\n",
    "        self.writer = SummaryWriter(writer_log_dir)\n",
    "\n",
    "    def train(self, loss_fn, optimizer):\n",
    "        ap_curve = BinaryAveragePrecision()\n",
    "\n",
    "        self.writer.add_text(\"args\", args.print())\n",
    "\n",
    "        with tqdm(total=args.epochs * 5, desc=\"training model...\") as pbar:\n",
    "            # cached values from previous epoch dataset\n",
    "            last_ds_labels = None\n",
    "            last_train_dataloader = None\n",
    "\n",
    "            for epoch in range(args.epochs):\n",
    "                pbar.set_description(f\"epoch {epoch + 1}\")\n",
    "\n",
    "                ## EPOCH DATASET GENERATION\n",
    "\n",
    "                # generate subgraph dataset\n",
    "                sampled_nodes = self.graph.subgraph(\n",
    "                    args.subsample,\n",
    "                    args.subsample_ranking,\n",
    "                    args.subsample_size\n",
    "                )\n",
    "                sampled_node_pairs = list(itertools.combinations(sampled_nodes, 2))\n",
    "                values = [ e.flatten().tolist() for e in np.array(self.graph.nodes)[sampled_node_pairs]]\n",
    "                labels = [ int(self.graph.adjacency_matrix[s]) for s in sampled_node_pairs ]\n",
    "                ds_values = torch.FloatTensor(values).to(device)\n",
    "                ds_labels = torch.IntTensor(labels).to(device)\n",
    "                ds = TensorDataset(ds_values, ds_labels)\n",
    "                train_dataloader = DataLoader(ds, batch_size=args.batch_size)\n",
    "                pbar.update(1)\n",
    "\n",
    "                # optional balancing of the dataloader that keeps labels in balance\n",
    "                if args.balance:\n",
    "                    labels_unique, labels_count = np.unique(ds_labels.cpu(), return_counts=True) # independent of train-test split (has both classes)\n",
    "                    labels_weights = [\n",
    "                        (1 / labels_count[int(l)])\n",
    "                        for _, l in ds\n",
    "                    ]\n",
    "                    edge_sampler = WeightedRandomSampler(labels_weights, len(labels_weights), replacement=True)\n",
    "                    train_dataloader = DataLoader(ds, batch_size=args.batch_size, sampler=edge_sampler)\n",
    "\n",
    "                ## EPOCH TRAINING\n",
    "\n",
    "                # train on batches\n",
    "                train_losses = []\n",
    "                train_preds = []\n",
    "                self.net.train()\n",
    "                for i_batch, (x_train, y_train) in enumerate(train_dataloader):\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = self.net(x_train)\n",
    "                    loss = loss_fn(y_pred, y_train.unsqueeze(1).float())\n",
    "                    loss.backward() # is this right?!\n",
    "                    optimizer.step()\n",
    "                    train_preds.extend(y_pred.unsqueeze(1))\n",
    "                    train_losses.append(loss.item())\n",
    "                pbar.update(1)\n",
    "\n",
    "                ## EPOCH EVALUATION\n",
    "\n",
    "                self.net.eval()\n",
    "\n",
    "                # evaluate epoch training on subgraph with basic metrics\n",
    "                mean_train_loss = np.mean(train_losses)\n",
    "                self.writer.add_scalar('mean_train_loss', mean_train_loss, epoch)\n",
    "                ap_score = ap_curve(torch.FloatTensor(train_preds).to(device), ds_labels)\n",
    "                self.writer.add_scalar('average_train_precision', ap_score, epoch)\n",
    "\n",
    "                # evaluate epoch after training on subgraph with basic metrics\n",
    "                test_preds = []\n",
    "                for i_batch, (x_train, y_train) in enumerate(train_dataloader):\n",
    "                    y_pred = self.net(x_train)\n",
    "                    test_preds.extend(y_pred.unsqueeze(1))\n",
    "                ap_score = ap_curve(torch.FloatTensor(test_preds).to(device), ds_labels)\n",
    "                self.writer.add_scalar('average_test_precision', ap_score, epoch)\n",
    "                pbar.update(1)\n",
    "\n",
    "                # evaluate epoch after training on previous subgraph with basic metrics\n",
    "                if epoch > 0:\n",
    "                    re_test_preds = []\n",
    "                    for i_batch, (x_train, y_train) in enumerate(last_train_dataloader):\n",
    "                        y_pred = self.net(x_train)\n",
    "                        re_test_preds.extend(y_pred.unsqueeze(1))\n",
    "                    ap_score = ap_curve(torch.FloatTensor(re_test_preds).to(device), last_ds_labels)\n",
    "                    self.writer.add_scalar('average_re_test_precision', ap_score, epoch)\n",
    "                last_train_dataloader = train_dataloader\n",
    "                last_ds_labels = ds_labels\n",
    "                pbar.update(1)\n",
    "\n",
    "                # evaluate epoch on whole graph after training (very expensive)\n",
    "                # is done at least at the end or is set, every 10 epochs\n",
    "                if (args.eval_epochs and epoch % 10 == 0) or epoch == args.epochs - 1:\n",
    "                    # print characteristics of used subgraph\n",
    "                    n_sampled_nodes = len(sampled_nodes)\n",
    "                    n_labels = len(labels)\n",
    "                    n_labels_0 = labels.count(0)\n",
    "                    self.writer.add_text(\n",
    "                        \"subgraph_sample\",\n",
    "                        f\"\"\"\n",
    "                            nodes: {n_sampled_nodes} / {self.graph.n_nodes}\n",
    "                            edges: {n_labels - n_labels_0} / {self.n_labels - self.n_labels_0}\n",
    "                            non-edges: {n_labels_0} / {self.n_labels_0}\n",
    "                        \"\"\",\n",
    "                        epoch\n",
    "                    )\n",
    "                    # evaluate on whole graph\n",
    "                    out = self.net(self.ds_values.to(device)) # TODO use batching\n",
    "                    raw_pred = torch.sigmoid(out)\n",
    "                    pred = [el.squeeze().tolist() for el in raw_pred]\n",
    "\n",
    "                    # save confusion matrix\n",
    "                    bcm = BinaryConfusionMatrix(validate_args=True).to(device)\n",
    "                    conf = bcm(torch.tensor(pred).to(device), self.ds_labels)\n",
    "                    self.writer.add_text(\n",
    "                        \"graph_confusion\",\n",
    "                        f\"\"\"\n",
    "                            non-edge | right: {conf[0][0]} / wrong: {conf[0][1]} / all: {self.n_labels_0}\n",
    "                            edge | right: {conf[1][1]} / wrong: {conf[1][0]} / all: {self.n_labels - self.n_labels_0}\n",
    "                        \"\"\",\n",
    "                        epoch\n",
    "                    )\n",
    "                    #self.writer.add_scalars('confusion', {\n",
    "                    #    'false positive edge': conf[0][1] / self.n_labels_0,\n",
    "                    #    'false negative edge': conf[1][0] / (self.n_labels - self.n_labels_0),\n",
    "                    #}, epoch)\n",
    "\n",
    "                    # save figure of reconstruction\n",
    "                    f = self.__print_graph(pred, subgraph=sampled_nodes)\n",
    "                    self.writer.add_figure('predicted graph', f, epoch)\n",
    "                pbar.update(1)\n",
    "\n",
    "    def test(self):\n",
    "        self.net.eval()\n",
    "        all_pred_label = []\n",
    "        all_label = []\n",
    "        with torch.no_grad():\n",
    "            test_dataloader = DataLoader(self.dataset, batch_size=args.batch_size)\n",
    "            for i_batch, (x_test, y_test) in enumerate(test_dataloader):\n",
    "                y_pred = self.net(x_test)\n",
    "                pred = torch.sigmoid(y_pred)\n",
    "                pred_label = torch.round(pred)\n",
    "                all_pred_label.append(pred_label.cpu().numpy())\n",
    "                all_label.append(y_test.cpu().numpy())\n",
    "\n",
    "        all_pred_label = sum([el.squeeze().tolist() for el in all_pred_label], [])\n",
    "        all_label = sum([el.squeeze().tolist() for el in all_label], [])\n",
    "\n",
    "        #confusion_matrix(all_label, all_pred_label)\n",
    "        print(classification_report(all_label, all_pred_label))\n",
    "\n",
    "\n",
    "    def evaluate(self, print_report: bool = True, print_graph: bool = True):\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.net(self.ds_values.to(device))\n",
    "            pred = torch.sigmoid(out)\n",
    "            pred_label = torch.round(pred)\n",
    "\n",
    "            if print_report:\n",
    "                #? how many wrongly predicted true/false\n",
    "                print(classification_report(\n",
    "                    self.ds_labels.cpu().numpy(),\n",
    "                    pred_label.cpu(),\n",
    "                    labels=[0, 1]\n",
    "                ))\n",
    "\n",
    "            _fig = None\n",
    "            if print_graph:\n",
    "                _pred = [el.squeeze().tolist() for el in pred]\n",
    "                _fig = self.__print_graph(_pred)\n",
    "\n",
    "            return out, pred, pred_label, _fig\n",
    "\n",
    "\n",
    "    def __print_graph(self, pred: List[float], subgraph = None, save_fig: bool = False):\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        node_size = 2000 / args.graph_size\n",
    "\n",
    "        nodes_dict = list_to_dict(self.graph.nodes)\n",
    "\n",
    "        # generate embedding graph\n",
    "        embed_graph = nx.Graph()\n",
    "        embed_graph.add_nodes_from(range(self.graph.n_nodes))\n",
    "        sources, targets = self.graph.adjacency_matrix.nonzero()\n",
    "        edges = zip(sources.tolist(), targets.tolist())\n",
    "        embed_graph.add_edges_from(edges)\n",
    "\n",
    "        # color subgraph nodes\n",
    "        node_colors = ['blue'] * self.graph.n_nodes\n",
    "        if subgraph is not None:\n",
    "            for n in subgraph:\n",
    "                node_colors[n] = 'green'\n",
    "\n",
    "        # print embed graph\n",
    "        ax[0].set_axis_off()\n",
    "        ax[0].set_aspect('equal')\n",
    "        ax[0].set_title(\"original graph\")\n",
    "        nx.draw_networkx(embed_graph, pos=nodes_dict, ax=ax[0], node_size=node_size, with_labels=False, labels={}, node_color=node_colors)\n",
    "\n",
    "        # generate predict graph\n",
    "        colors_filtered = np.array([\n",
    "            pred[i]\n",
    "            for i, _ in enumerate(self.graph.node_index_pairs)\n",
    "            if pred[i] > 0.5\n",
    "        ])\n",
    "        colormap = sns.color_palette(\"flare\", as_cmap=True)\n",
    "        pred_graph = nx.Graph()\n",
    "        pred_graph.add_nodes_from(range(self.graph.n_nodes))\n",
    "        pred_graph.add_edges_from([\n",
    "            pair\n",
    "            for i, pair in enumerate(self.graph.node_index_pairs)\n",
    "            if pred[i] > 0.5\n",
    "        ])\n",
    "\n",
    "        # print predicted graph\n",
    "        ax[1].set_axis_off()\n",
    "        ax[1].set_aspect('equal')\n",
    "        ax[1].set_title(\"reconstructed graph\")\n",
    "        nx.draw_networkx(pred_graph, pos=nodes_dict, ax=ax[1], node_size=node_size, with_labels=False, labels={}, edge_color=colors_filtered, edge_cmap=colormap)\n",
    "\n",
    "        # add color bar for predictions\n",
    "        cax = fig.add_axes([ax[1].get_position().x1 + 0.01, ax[1].get_position().y0, 0.02, ax[1].get_position().height])\n",
    "        fig.colorbar(mpl.cm.ScalarMappable(cmap=colormap), cax=cax, label=\"confidence\")\n",
    "\n",
    "        if save_fig:\n",
    "            plt.savefig('./filename.png', dpi=300)\n",
    "        return fig"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "bfs_args = Args()\n",
    "bfs_args.subsample = \"bfs\"\n",
    "bfs_args.subsample_size = 100\n",
    "bfs_args.eval_epochs = False\n",
    "\n",
    "bfsl_args = Args()\n",
    "bfsl_args.subsample = \"bfs\"\n",
    "bfsl_args.subsample_size = 250\n",
    "bfsl_args.eval_epochs = False\n",
    "\n",
    "walk_args = Args()\n",
    "walk_args.subsample = \"random_walk\"\n",
    "walk_args.subsample_size = 100\n",
    "walk_args.eval_epochs = False\n",
    "\n",
    "walk_l_args = Args()\n",
    "walk_l_args.subsample = \"random_walk\"\n",
    "walk_l_args.subsample_size = 250\n",
    "walk_l_args.eval_epochs = False\n",
    "\n",
    "walk_lu_args = Args()\n",
    "walk_lu_args.subsample = \"random_walk\"\n",
    "walk_lu_args.subsample_size = 250\n",
    "walk_lu_args.subsample_ranking = \"uniform\"\n",
    "walk_lu_args.eval_epochs = False\n",
    "\n",
    "random_args = Args()\n",
    "random_args.subsample = \"random\"\n",
    "random_args.subsample_size = 100\n",
    "random_args.eval_epochs = False\n",
    "\n",
    "random_u_args = Args()\n",
    "random_u_args.subsample = \"random\"\n",
    "random_u_args.subsample_size = 100\n",
    "random_u_args.subsample_ranking = \"uniform\"\n",
    "random_u_args.eval_epochs = False\n",
    "\n",
    "random_l_args = Args()\n",
    "random_l_args.subsample = \"random\"\n",
    "random_l_args.subsample_size = 250\n",
    "random_l_args.eval_epochs = False\n",
    "\n",
    "random_lu_args = Args()\n",
    "random_lu_args.subsample = \"random\"\n",
    "random_lu_args.subsample_size = 250\n",
    "random_lu_args.subsample_ranking = \"uniform\"\n",
    "random_lu_args.eval_epochs = False\n",
    "\n",
    "\n",
    "_args = [\n",
    "    #bfs_args,\n",
    "    #bfs_l_args,\n",
    "    #walk_args,\n",
    "    #walk_l_args,\n",
    "    #walk_lu_args,\n",
    "    random_args,\n",
    "    random_u_args,\n",
    "    #random_l_args,\n",
    "    #random_lu_args,\n",
    "]\n",
    "_rep = 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "generating random-uniform nodes on disc:   0%|          | 0/800 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13356a58dc534014a848f609c872c790"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "training model...:   0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abc219eb736a4fafaaa613ac496b3f73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculating node_degree\n"
     ]
    },
    {
     "data": {
      "text/plain": "generating random-uniform nodes on disc:   0%|          | 0/800 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cba866f2c98849ae871ee645033870ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "training model...:   0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4dc246d2f0d84285860e83353d31e480"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculating node_degree\n"
     ]
    },
    {
     "data": {
      "text/plain": "generating random-uniform nodes on disc:   0%|          | 0/800 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "535b225f76cb4f368d332c6179ca02ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "training model...:   0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8fea8bf2daa4f9cb473177b469d16f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculating node_degree\n"
     ]
    },
    {
     "data": {
      "text/plain": "generating random-uniform nodes on disc:   0%|          | 0/800 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2067aa1021034c74b5d47594b6db3839"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "training model...:   0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4be02972470c44b8ba5b7bee95d33bfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculating uniform\n"
     ]
    },
    {
     "data": {
      "text/plain": "generating random-uniform nodes on disc:   0%|          | 0/800 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "718a99e763ac4c7e94b7c9ed451ad978"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "training model...:   0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf123fd0897f43c3a8defe721fc30004"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculating uniform\n"
     ]
    },
    {
     "data": {
      "text/plain": "generating random-uniform nodes on disc:   0%|          | 0/800 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e5040bf681d4ed88d84e990592208b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "training model...:   0%|          | 0/500 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ecd638c3d6b449799eb851b763c2bf6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculating uniform\n"
     ]
    }
   ],
   "source": [
    "evaluator = None\n",
    "graph_builder = None\n",
    "\n",
    "# generate readable key for runs\n",
    "now = datetime.datetime.now()\n",
    "runs_dir = \"runs\"\n",
    "experiment_key = now.strftime(\"%d-%m--%H-%M\")\n",
    "\n",
    "for a in _args:\n",
    "    args = a\n",
    "    for r in range(_rep):\n",
    "        # build and run evaluator\n",
    "        graph_builder = RandomGeometricGraphBuilder()\n",
    "        graph = Graph(\n",
    "            graph_builder.nodes,\n",
    "            graph_builder.adjacency_matrix,\n",
    "        )\n",
    "        evaluator = EmbeddingEvaluator(\n",
    "            graph,\n",
    "            writer_log_dir=f\"{runs_dir}/{experiment_key}/{args.subsample}-{args.subsample_size}-{args.subsample_ranking}--{r}\"\n",
    "        )\n",
    "        evaluator.train(\n",
    "            loss_fn=nn.BCEWithLogitsLoss(),\n",
    "            optimizer=torch.optim.Adam(evaluator.net.parameters(), lr=1e-3)\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator.test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate embedding\n",
    "_, eval_pred, eval_pred_label, figure = evaluator.evaluate()\n",
    "figure.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print edge distance to prediction\n",
    "\n",
    "# generate 10x edges per distance\n",
    "dist_edges = [\n",
    "    (d, [\n",
    "        gen_disc_edge(d)\n",
    "        for _ in range(8)\n",
    "    ])\n",
    "    for d in np.arange(0, 1, 0.001)\n",
    "]\n",
    "\n",
    "xs = []\n",
    "ys_mean = []\n",
    "ys_std = []\n",
    "\n",
    "evaluator.net.eval()\n",
    "with torch.no_grad():\n",
    "    for d, edges in dist_edges:\n",
    "        out = evaluator.net(torch.FloatTensor(edges).to(device))\n",
    "        pred = torch.sigmoid(out)\n",
    "        pred = [el.squeeze().tolist() for el in pred]\n",
    "        xs.append(d)\n",
    "        ys_mean.append(np.mean(pred))\n",
    "        ys_std.append(np.std(pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.scatter(xs, ys_mean)\n",
    "plt.errorbar(xs, ys_mean, xerr=ys_std, xlolims=[std > 0.01 for std in ys_std], linestyle=\"None\")\n",
    "plt.axvline(x=args.rg_radius, color='green')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.scatter(xs, ys_mean)\n",
    "#plt.errorbar(xs, ys_mean, xerr=ys_std, xlolims=[std > 0.01 for std in ys_std], linestyle=\"None\")\n",
    "plt.axvline(x=args.rg_radius, color='green')\n",
    "\n",
    "plt.xlim([0.12, 0.25])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(self, print_report: bool = True, print_graph: bool = True, th=0.5):\n",
    "    self.net.eval()\n",
    "    with torch.no_grad():\n",
    "        out = self.net(self.ds_values.to(device))\n",
    "        pred = torch.sigmoid(out)\n",
    "        pred_label = torch.round(pred)\n",
    "\n",
    "        if print_report:\n",
    "            #? how many wrongly predicted true/false\n",
    "            print(classification_report(\n",
    "                self.ds_labels.cpu().numpy(),\n",
    "                pred_label.cpu(),\n",
    "                labels=[0, 1]\n",
    "            ))\n",
    "\n",
    "        if print_graph:\n",
    "            _pred = [el.squeeze().tolist() for el in pred]\n",
    "            __print_graph(self, _pred, th=th)\n",
    "\n",
    "        return out, pred, pred_label\n",
    "\n",
    "\n",
    "def __print_graph(self, pred: List[float], th=0.5):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    nodes_dict = list_to_dict(self.nodes)\n",
    "\n",
    "    # generate embedding graph\n",
    "    embed_graph = nx.Graph()\n",
    "    embed_graph.add_nodes_from(range(self.n_nodes))\n",
    "    embed_graph.add_edges_from(self.edges)\n",
    "\n",
    "    # print embed graph\n",
    "    ax[0].set_axis_off()\n",
    "    ax[0].set_aspect('equal')\n",
    "    ax[0].set_title(\"original graph\")\n",
    "    nx.draw_networkx(embed_graph, pos=nodes_dict, ax=ax[0], node_size=5, with_labels=False, labels={})\n",
    "\n",
    "    # generate predict graph\n",
    "    colors_filtered = np.array([\n",
    "        pred[i]\n",
    "        for i, _ in enumerate(self.node_index_pairs)\n",
    "        if pred[i] > th\n",
    "    ])\n",
    "    colormap = sns.color_palette(\"flare\", as_cmap=True)\n",
    "    pred_graph = nx.Graph()\n",
    "    pred_graph.add_nodes_from(range(self.n_nodes))\n",
    "    pred_graph.add_edges_from([\n",
    "        pair\n",
    "        for i, pair in enumerate(self.node_index_pairs)\n",
    "        if pred[i] > th\n",
    "    ])\n",
    "\n",
    "    # print predicted graph\n",
    "    ax[1].set_axis_off()\n",
    "    ax[1].set_aspect('equal')\n",
    "    ax[1].set_title(\"reconstructed graph\")\n",
    "    nx.draw_networkx(pred_graph, pos=nodes_dict, ax=ax[1], node_size=5, with_labels=False, labels={}, edge_color=colors_filtered, edge_cmap=colormap)\n",
    "\n",
    "    # add color bar for predictions\n",
    "    cax = fig.add_axes([ax[1].get_position().x1 + 0.01, ax[1].get_position().y0, 0.02, ax[1].get_position().height])\n",
    "    fig.colorbar(mpl.cm.ScalarMappable(cmap=colormap), cax=cax, label=\"confidence\")\n",
    "\n",
    "    plt.savefig('./filename.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "evaluate(evaluator, th=0.93)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
